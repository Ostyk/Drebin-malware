from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import StratifiedKFold,KFold
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score
from sklearn.metrics import classification_report, recall_score, f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier


import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
import time
from uncertainties import unumpy


def scale_set(train,test):
    """uses sklearn standard sclar to normalize data"""
    sc = StandardScaler()
    return sc.transform(train), sc.transform(test)



def clssifier_type(X_train, X_test,y_train, y_test, p1=None, p2=None, classifier='SVM'):
    ''' Cross validation of different alogirthms
    p1, p2 - are hyperparameters
    '''
    if classifier=='SVM':
        clf = SVC(probability=True, C=p1,gamma=p2)
    elif classifier=='RANDOM-FOREST':
        clf = RandomForestClassifier(n_estimators=p1, max_depth=p2,random_state=0)

    clf.fit(X_train, y_train)
    y_predict=clf.predict(X_test)  
    #print(y_predict)
    if len(np.unique(y_test))>2:
        #print("hello:")
        t_= 'macro'
    else:
        t_='weighted'
    acc = accuracy_score(y_test, y_predict)
    pre = precision_score(y_test, y_predict,average=t_)
    rec = recall_score(y_test, y_predict,average=t_)
    f1 = f1_score(y_test, y_predict, average=t_, labels=np.unique(y_predict))
    
    x = {"accuracy":acc,
            "recall":rec,
            "precision":pre,
            "f1-score":f1}

    y_score = clf.predict_proba(X_test)
    fpr, tpr, thresholds = roc_curve(y_test, y_score[:, 1])
    roc = [fpr, tpr]
    return pd.DataFrame(list(x.values()),index=list(x.keys()),columns=['model']).T, roc

def model(X, y, n_splits=5, seed=10,model_name='SVM',balanced=True):
    maximising_score = 'accuracy'
    if not balanced:
        maximising_score = 'f1-score'
    '''
    Args: X-data, y-labels,
    n_splits - K-fold cross validation splits

    '''
    if model_name=='SVM':
        hp1 = [10, 100, 1000] #C
        hp2 = [1e-2, 1e-3, 1e-4] # Gamma
    elif model_name=='RANDOM-FOREST':
        hp1 = [100,200,300] #n_estimators
        hp2 = [2,3,4] #max_depth


    permutations = [(x,y) for x in hp2 for y in hp1]

    kf = StratifiedKFold(n_splits=n_splits,random_state=42, shuffle=True)
    np.random.seed(seed)
    baseline = pd.DataFrame(np.zeros(4)).T
    baseline.columns = ['accuracy','recall','precision','f1-score']

    for p1,p2 in permutations:
        #print("Gamma: {}, C = {}".format(p2,p1))
        empty = pd.DataFrame([],columns=['accuracy','recall','precision','f1-score'])

        tprs, aucs, mean_fpr = [], [], np.linspace(0, 1, 100)

        for train_index, test_index in kf.split(X,y):

            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            #X_train, X_test = scale_set(X_train,X_test)

            #metrics calcs
            performance, roc = clssifier_type(X_train, X_test, y_train, y_test, p1, p2, classifier=model_name)
            empty = pd.concat([empty,performance])
            #print(performance)
            #roc curve calcs
            tprs.append(scipy.interp(mean_fpr, roc[0], roc[1]))
            tprs[-1][0] = 0.0
            roc_auc = auc(roc[0], roc[1])
            aucs.append(roc_auc)

        permuation_performance = empty.mean() #mean of scores for all CVs

        if np.array([permuation_performance[maximising_score]])>=np.array([baseline[maximising_score]]): #maximizing f1 score
            baseline = pd.DataFrame(unumpy.uarray(permuation_performance, empty.std())).T

            baseline.columns = ['accuracy','recall','precision','f1-score']
            baseline['param 1'],baseline['param 2'] = p1, p2
            tprs_best = tprs
            aucs_best = aucs
            mean_fpr_best = mean_fpr

    r = {"tprs":tprs_best,
         "aucs":aucs_best,
         "mean_fpr":mean_fpr_best,
         "model_name":model_name} #roc

    #returning part
    baseline.insert(0, 'Model', model_name)

    return baseline,r

def plot_roc(rocs,balanced=True):
    '''plots the roc curve for a given model'''
    plt.figure(figsize=(10,7))
    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='cyan',
         label='Luck', alpha=.8)
    colors = ['b','k','r']
    for index, values in enumerate(rocs):
        tprs = values['tprs']
        aucs = values['aucs']
        mean_fpr = values['mean_fpr']
        model_name1 = values['model_name']

        mean_tpr = np.mean(tprs, axis=0)
        mean_tpr[-1] = 1.0
        mean_auc = auc(mean_fpr, mean_tpr)
        std_auc = np.std(aucs)
        plt.plot(mean_fpr, mean_tpr,
                 label='{} - AUC = {:.4f} $\pm$ {:.4f}'.format(model_name1, mean_auc, std_auc),
                 lw=2, alpha=.8,color=colors[index])

        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
        plt.fill_between(mean_fpr, tprs_lower, tprs_upper, alpha=.2,color=colors[index])


    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    #plt.title('Receiver operating characteristic of the {} model'.format(mode_name))
    plt.legend(loc="lower right")
    plt.show()
